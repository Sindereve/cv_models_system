import mlflow
from tqdm import tqdm
import torch
from torch import nn
from torch import optim
from torch.optim import Optimizer, lr_scheduler
from torch.utils.data import DataLoader

import time
from typing import Optional

class BaseTrainer:
    def __init__(
            self, 
            model: nn.Module,
            train_loader: DataLoader,
            val_loader: DataLoader,
            loss_fn: Optional[nn.Module] = None,
            optimizer: Optional[Optimizer] = None,
            scheduler: Optional[lr_scheduler._LRScheduler] = None,
            device: Optional[torch.device] = None,
            # next arg mlflow module
            log_mlflow: bool = True,
            log_artifacts: bool = True,
            experiment_name: str = "Experiment_name",
            run_name : Optional[str] = None,
        ):
        """
        Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‚Ñ€ĞµĞ½ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
        
        Args:
            model: ĞĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ ÑĞµÑ‚ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ
            train_loader: Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ
            val_loader: Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸
            loss_fn: Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ
            optimizer: ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€
            scheduler: ĞŸĞ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº learning rate (optional)
            device: Ğ£ÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ GPU\CPU
            log_mlflow: Ğ¤Ğ»Ğ°Ğ³ Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² MLflow
            log_artifacts: Ğ›Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ²
            experiment_name: Ğ˜Ğ¼Ñ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ° Ğ² MLflow
            run_name: Ğ£Ğ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¸Ğ¼Ñ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ° Ğ² MLflow
        """
        self._validate_input(model, train_loader, val_loader)
        print("âšª Start init")
        
        self.model = model
        self.train_loader = train_loader
        print(" â– Train load sample:", len(self.train_loader.dataset))
        self.val_loader = val_loader
        print(" â– Val load sample:  ", len(self.val_loader.dataset))

        # device
        self._setup_device(device)
        self.model.to(self.device)

        # loss and optimizer
        self.loss_fn = loss_fn or nn.CrossEntropyLoss()
        self.optimizer = optimizer or optim.Adam(self.model.parameters(), lr=0.001)
        self.scheduler = scheduler or lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=50)

        # metrics
        self.history = {
            'train_loss': [], 'train_accuracy': [],
            'val_loss': [], 'val_accuracy': [],
            'learning_rate': []
        }
        self.best_weights = None
        self.best_accuracy = 0.0
        
        # mlflow
        self.log_mlflow = log_mlflow
        self.log_artifacts = log_artifacts
        self._setup_mlflow(log_mlflow, experiment_name, run_name)

        print("ğŸŸ¢ Finish init")

    def _validate_input(
            self, 
            model: nn.Module, 
            train_loader: DataLoader, 
            val_loader: DataLoader
        ):
        """
        Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
        """
        if not isinstance(model, nn.Module):
            raise TypeError("model must be nn.Module")
        if not isinstance(train_loader, DataLoader):
            raise TypeError("train_loader must be DataLoader")
        if not isinstance(val_loader, DataLoader):
            raise TypeError("val_loader must be DataLoader")

    def _setup_device(self, device: Optional[torch.device] = None):
        """
        ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ
        """
        if device is None:
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = device
        
        if self.device.type == 'cuda' and not torch.cuda.is_available():
            print("ğŸŸ  Ğ’Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ: Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ 'CUDA', Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ 'CPU'")
            self.device = torch.device('cpu')
        torch.cuda.empty_cache()
        print(" â– Training on:", self.device)
        

    def _setup_mlflow(
            self,
            log_mlflow: bool,
            experiment_name: str,
            run_name: str,
        ):
        """
        ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° MLFlow ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ° Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¾Ğ¹ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº
        """
        if not log_mlflow:
            print(" â– log in Mlflow: OFF")
            return

        try:
            self.run_name = run_name
            self.experiment_name = experiment_name

            # ĞŸÑ‹Ñ‚Ğ°ĞµĞ¼ÑÑ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚, ĞµÑĞ»Ğ¸ Ğ½Ğµ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ - ÑĞ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ½Ğ¾Ğ²Ñ‹Ğ¹
            try:
                mlflow.set_experiment(self.experiment_name)
            except:
                # Ğ•ÑĞ»Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚ ÑƒĞ´Ğ°Ğ»ĞµĞ½, ÑĞ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼ĞµÑ‚ĞºĞ¾Ğ¹
                self.experiment_name = f"{experiment_name}_new_{int(time.time())}"
                mlflow.create_experiment(self.experiment_name)
                mlflow.set_experiment(self.experiment_name)
                print(f"ğŸ”µ[MLFlow] Created new experiment: {self.experiment_name}")

            if self.run_name is None:
                time_str = time.strftime('%Y:%m:%d_%H:%M:%S')
                self.run_name = f"{self.model.__class__.__name__}_{time_str}"

            self.mlflow_run = mlflow.start_run(run_name=self.run_name)
            self._log_model_parameters()
            print(" â– log in Mlflow: On")
            
        except Exception as e:
            print("ğŸ”´[MLFlow] Error setting:", e)
            self.log_mlflow = False
            try:
                mlflow.end_run()
            except:
                pass
            print("ğŸŸ [MLFlow] Continuing without MLflow logging")

    def _log_model_parameters(self):
        """
        Ğ›Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ
        """
        try: 
            model_params = {
                'model_type': self.model.__class__.__name__,
                'device': self.device.type,
                'total_parameters': sum([p.numel() for p in self.model.parameters()])
            }

            optiizer_params = {
                'optimizer': self.optimizer.__class__.__name__,
                'learning_rate': self.optimizer.param_groups[0]['lr']
            }

            for key, value in self.optimizer.param_groups[0].items():
                if key != 'params':
                    optiizer_params[f'optimizer_{key}'] = value
            
            data_params = {
                'train_sample': len(self.train_loader.dataset),
                'val_sample': len(self.val_loader.dataset),
                'batch_size': self.train_loader.batch_size,
                'num_classes': getattr(self.train_loader, 'num_classes', 'unknown')
            }
            
            all_params = {
                **model_params, 
                **data_params,
                **optiizer_params, 
                
            }
            mlflow.log_params(all_params)
            print('ğŸ”µ[MLFlow] parameters model add in MLFlow')
        except Exception as e:
            print("ğŸ”´[MLFlow] Error set params model:", e)
            raise


    def _log_epoch_metric(
            self, 
            epoch: int
        ):
        """
        Ğ›Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº ÑĞ¿Ğ¾Ñ…Ğ¸ Ğ² MLflow
        """
        if not self.log_mlflow:
            return
        try:
            metrics = {
                'train_loss': self.history['train_loss'][-1],
                'train_accuracy': self.history['train_accuracy'][-1],
                'val_loss': self.history['val_loss'][-1],
                'val_accuracy': self.history['val_accuracy'][-1],
                'learning_rate': self.history['learning_rate'][-1],
                'epoch': epoch
            }

            mlflow.log_metrics(metrics, step=epoch)

        except Exception as e:
            print("ğŸ”´[MLFlow] Error set params model:", e)


    def _log_model_checkpoint(self, epoch: int, is_best: bool = False):
        """
        Ğ›Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‡ĞµĞºĞ¿Ğ¾Ğ¸Ğ½Ñ‚Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
        """
        if not self.log_mlflow or not self.log_artifacts:
            return
            
        try:
            # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ
            model_info = mlflow.pytorch.log_model(
                self.model,
                "model",
                registered_model_name=f"{self.model.__class__.__name__}",
            )
            
            # Ğ•ÑĞ»Ğ¸ ÑÑ‚Ğ¾ Ğ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ»Ğ¾Ğ³Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾
            if is_best:
                mlflow.pytorch.log_model(
                    self.model,
                    "best_model",
                    registered_model_name=f"{self.model.__class__.__name__}_best",
                )
                mlflow.log_metric("best_val_accuracy", self.history['val_accuracy'][-1])
                
        except Exception as e:
            print(f"ğŸ”´[MLFlow] Error logging model: {e}")

    def _log_training_artifacts(self):
        """
        Ğ›Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ²
        """
        if not self.log_mlflow or not self.log_artifacts:
            return
            
        try:
            import matplotlib.pyplot as plt
            import os
            import tempfile
            
            # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ´Ğ»Ñ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ²
            with tempfile.TemporaryDirectory() as temp_dir:
                
                # Ğ“Ñ€Ğ°Ñ„Ğ¸Ğº Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ
                plt.figure(figsize=(12, 4))
                
                plt.subplot(1, 2, 1)
                plt.plot(self.history['train_loss'], label='Train Loss')
                plt.plot(self.history['val_loss'], label='Val Loss')
                plt.title('Model Loss')
                plt.xlabel('Epoch')
                plt.ylabel('Loss')
                plt.legend()
                plt.grid(True)
                
                plt.subplot(1, 2, 2)
                plt.plot(self.history['train_accuracy'], label='Train Accuracy')
                plt.plot(self.history['val_accuracy'], label='Val Accuracy')
                plt.title('Model Accuracy')
                plt.xlabel('Epoch')
                plt.ylabel('Accuracy')
                plt.legend()
                plt.grid(True)
                
                plt.tight_layout()
                loss_plot_path = os.path.join(temp_dir, 'training_metrics.png')
                plt.savefig(loss_plot_path)
                plt.close()
                
                mlflow.log_artifact(loss_plot_path)
                
                # Ğ›Ğ¾Ğ³Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ñ„Ğ°Ğ¹Ğ»
                history_path = os.path.join(temp_dir, 'training_history.txt')
                with open(history_path, 'w') as f:
                    f.write("Epoch\tTrain_Loss\tTrain_Acc\tVal_Loss\tVal_Acc\tLR\n")
                    for i in range(len(self.history['train_loss'])):
                        f.write(f"{i+1}\t{self.history['train_loss'][i]:.4f}\t"
                               f"{self.history['train_accuracy'][i]:.4f}\t"
                               f"{self.history['val_loss'][i]:.4f}\t"
                               f"{self.history['val_accuracy'][i]:.4f}\t"
                               f"{self.history['learning_rate'][i]:.6f}\n")
                
                mlflow.log_artifact(history_path)
                
        except Exception as e:
            print(f"ğŸ”´[MLFlow] Error logging artifacts: {e}")


    def _train_one_epoch(
            self,
        ):
        """
        ĞŸÑ€Ğ¾Ñ…Ğ¾Ğ´ Ğ¿Ğ¾ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¸ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ½Ğ° Ğ½Ğ¸Ñ…
        """

        self.model.train()

        runner_loss = 0.0
        correct_predictions = 0
        total_sample = 0

        for data in self._tqdm_loader(self.train_loader, "Training"):
            inputs, labels = data
            
            inputs = inputs.to(self.device)
            labels = labels.to(self.device)

            self.optimizer.zero_grad()

            # front steps
            outputs = self.model(inputs)
            loss = self.loss_fn(outputs, labels)
            
            # back steps
            loss.backward()
            self.optimizer.step()

            runner_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total_sample += labels.size(0)
            correct_predictions += (predicted == labels).sum().item()
            
            # cuda opyat ushla vsya pamyat'
            del inputs, labels, outputs, loss
        
        self.scheduler.step()

        epoch_loss = runner_loss / len(self.train_loader)
        epoch_accuracy = correct_predictions / total_sample
        lr = self.optimizer.param_groups[0]['lr']

        self.history['train_loss'].append(epoch_loss)
        self.history['train_accuracy'].append(epoch_accuracy)
        self.history['learning_rate'].append(lr)

        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        print(f"Epoch Result:")
        print(f" â– Train Loss: {epoch_loss:.4f}")
        print(f" â– Train Acc:  {epoch_accuracy:.4f}")
        print(f" â– LR:         {lr:.6f}")

    def _tqdm_loader(
            self,
            data_loader: DataLoader,
            desc: str = "process"
        ):
        """
        Ğ‘Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ´Ğ»Ñ ĞºÑ€Ğ°ÑĞ¸Ğ²Ğ¾Ğ³Ğ¾ Ğ±Ğ°Ñ€Ğ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸
        """
        return tqdm(
            data_loader,
            desc=desc,
            bar_format="{l_bar}{bar:20}{r_bar}",
            colour="blue",
            leave=False
        )

    def _validate_one(
            self
        ) -> None:
        """
        1 Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´ Ğ¿Ğ¾ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼
        """
        self.model.eval()

        runner_loss = 0.0
        correct_predictions = 0
        total_sample = 0

        with torch.no_grad():
            for data in self._tqdm_loader(self.val_loader, "Validating"):
                
                inputs, labels = data
                inputs = inputs.to(self.device)
                labels = labels.to(self.device)

                outputs = self.model(inputs)
                loss = self.loss_fn(outputs, labels)

                runner_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                total_sample += labels.size(0)
                correct_predictions += (predicted == labels).sum().item()
                
                # cude opyat ushla vsya pamyat'
                del inputs, outputs, labels, loss

        avg_loss = runner_loss / (len(self.val_loader))
        accuracy = correct_predictions / total_sample

        self.history["val_loss"].append(avg_loss)
        self.history["val_accuracy"].append(accuracy)

        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        print(f"Validat:")
        print(f" â– Val Loss: {avg_loss:.4f}")
        print(f" â– Val Acc:  {accuracy:.4f}")

    def train(
            self,
            epochs: int = 20,
        ) -> nn.Module:
        """
        ĞŸĞ¾Ğ»Ğ½Ñ‹Ğ¹ Ñ†Ğ¸ĞºĞ» Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸
        
        Args:
            epoch: ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑĞ¿Ğ¾Ñ… Ğ´Ğ»Ñ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸
        """

        print("ğŸ”˜[train] Start")

        for epoch in range(epochs):
            print("="*50)
            print(f"ğŸ”„ Epoch[ğŸ”¹{epoch+1}/{epochs}ğŸ”¹] start")
            self._train_one_epoch()
            self._validate_one()
            
            self._log_epoch_metric(epoch+1)
            is_best = self.history['val_accuracy'][-1] == self.best_accuracy
            if is_best:
                self._log_model_checkpoint(epoch + 1, is_best=True)

            print(f"âœ… Epoch[ğŸ”¹{epoch+1}/{epochs}ğŸ”¹] completed")

        # Ğ›Ğ¾Ğ³Ğ¸Ñ€ÑƒĞµĞ¼ Ğ²ÑĞµ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ñ‹
        self._log_training_artifacts()

        if self.best_weights is not None:
            self.model.load_state_dict(self.best_weights)

        if self.log_mlflow:
            mlflow.end_run()

        print("ğŸŸ¢[train] Completed!!!")