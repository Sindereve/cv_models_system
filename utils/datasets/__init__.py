import yaml
import torch
from torchvision import transforms, datasets
from torch.utils.data import DataLoader, Subset
from typing import Tuple, List

from .detection import DetectionDataset
from .tools import calculate_normalize_datasets, denormalize_image

__all__ = [denormalize_image, calculate_normalize_datasets]

def load_dataloader_classification(
        path_data_dir: str,
        img_w_size: int = 224,
        img_h_size: int = 224,
        total_img: int = 0,
        batch_size: int = 32,
        train_ration: float = 0.8,
        is_calculate_normalize_dataset: bool = True
    ) -> Tuple[DataLoader, DataLoader, List[str]]:
    """
    Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ‘Ğ¼ Dataloader

    Args:
        path_data_dir: Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ¿Ğ°Ğ¿ĞºĞµ Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸
        img_w_size: ÑˆĞ¸Ñ€Ğ¸Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹
        img_h_size: Ğ²Ñ‹ÑĞ¾Ñ‚Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹
        total_img: ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ½ÑƒĞ¶Ğ½Ğ¾
        batch_size: Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ±Ğ°Ñ‚Ñ‡ĞµĞ¹
        train_ration: Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğº Ğ²ÑĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼
        is_calculate_normalize_dataset: Ğ½ÑƒĞ¶Ğ½Ğ¾ Ğ»Ğ¸ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ°ÑˆĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ

    Returns:
        Dataloader: Dataloader Ğ´Ğ»Ñ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
        Dataloader: Dataloader Ğ´Ğ»Ñ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
        list[str]: ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğ¹ ĞºĞ»Ğ°ÑÑĞ¾Ğ²
    """
    print("âšª[load_dataloader_classification] start create dataloaders")

    base_transform = transforms.Compose([
        transforms.Resize((img_h_size, img_w_size)),
        transforms.ToTensor()
    ])

    temp_dataset = datasets.ImageFolder(
        root=path_data_dir,
        transform=base_transform
    )

    temp_loader = DataLoader(
        temp_dataset,
        batch_size=batch_size,
        shuffle=False,
    )

    classes = temp_loader.dataset.classes

    if is_calculate_normalize_dataset:
        print("ğŸŸ£[normalize_dataset] processing")
        mean, std = calculate_normalize_datasets(temp_loader)
    
        train_transform = transforms.Compose([
            transforms.RandomResizedCrop(
                size=(img_h_size, img_w_size),
                scale=(0.7, 1.0)
            ),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomRotation(10),
            transforms.ToTensor(),
            transforms.Normalize(mean=mean, std=std)
        ])

        val_transform = transforms.Compose([
            transforms.Resize(size=(img_h_size, img_w_size)),
            transforms.ToTensor(),
            transforms.Normalize(mean=mean, std=std)
        ])
    else:
        train_transform = transforms.Compose([
            transforms.RandomResizedCrop(
                size=(img_h_size, img_w_size),
                scale=(0.7, 1.0)
            ),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomRotation(10),
            transforms.ToTensor(),
        ])

        val_transform = transforms.Compose([
            transforms.Resize(size=(img_h_size, img_w_size)),
            transforms.ToTensor(),
        ])

    if total_img == 0:
        total_img = len(temp_dataset)

    indxs = torch.randperm(len(temp_dataset))[:total_img]
    temp_dataset = Subset(temp_dataset, indxs)

    train_size = int(train_ration * total_img)
    val_size = total_img - train_size

    train_dataset, val_dataset = torch.utils.data.random_split(
        temp_dataset, [train_size, val_size]
    )

    train_dataset.dataset.transform = train_transform
    val_dataset.dataset.transform = val_transform

    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
    )

    print("ğŸŸ¢[load_dataloader_classification] finish create dataloaders")
    print(f" â– Train samples: {len(train_dataset)}")
    print(f" â– Val samples:   {len(val_dataset)}")
    print(f" â– Classes:       {classes}")

    return train_loader, val_loader, classes

def load_dataloader_detection(
        path_data_dir: str,
        img_w_size: int = 224,
        img_h_size: int = 224,
        total_img: int = 0,
        batch_size: int = 32,
        train_ration: float = 0.8,
        verbose: bool = False
    ) -> Tuple[DataLoader, DataLoader, List[str]]:
    
    print("âšª[load_dataloader_detection] start create dataloaders")

    with open(path_data_dir+'/data.yaml', 'r') as f:
        config = yaml.safe_load(f)

    train_path =  config['train']
    val_path =  config['val']
    classes = config['names']

    train_dataset = DetectionDataset(
        images_dir=train_path,
        global_path=path_data_dir,
        img_size=(img_h_size, img_w_size)
        verbose=verbose
    )

    val_dataset = DetectionDataset(
        images_dir=val_path,
        global_path=path_data_dir,
        img_size=(img_h_size, img_w_size),
        verbose=verbose
    )

    # ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹
    if total_img > 0:
        train_count = int(total_img * train_ration)
        val_count = total_img - train_count
        
        train_count = min(train_count, len(train_dataset))
        val_count = min(val_count, len(val_dataset))
        
        # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ°
        train_indices = torch.randperm(len(train_dataset))[:train_count]
        val_indices = torch.randperm(len(val_dataset))[:val_count]
        
        train_dataset = Subset(train_dataset, train_indices)
        val_dataset = Subset(val_dataset, val_indices)
    else:
        train_dataset = train_dataset
        val_dataset = val_dataset

    def collate_fn(batch):
        images, labels = zip(*batch)
        images = torch.stack(images)
        return images, labels

    train_dataloader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        collate_fn=collate_fn
    )

    val_dataloader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        collate_fn=collate_fn
    )

    print("ğŸŸ¢[load_dataloader_detection] finish create dataloaders")
    print(f" â– Train samples: {len(train_dataset)}")
    print(f" â– Val samples:   {len(val_dataset)}")
    print(f" â– Classes:       {classes}")

    return train_dataloader, val_dataloader, classes