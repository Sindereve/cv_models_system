import os
import yaml
from pathlib import Path

import torch
from torchvision import datasets, transforms
from torch.utils.data import DataLoader, Subset, Dataset
from tqdm import tqdm
from PIL import Image
from typing import List, Tuple, Any
import glob

def load_dataloader(
        data_dir: str,
        img_w_size: int = 224,
        img_h_size: int = 224,
        total_img: int = 0,
        batch_size: int = 32,
        train_ration: float = 0.8,
        is_calculate_normalize_dataset: bool = True
    ) -> Tuple[DataLoader, DataLoader, List[str]]:
    """
    –°–æ–∑–¥–∞–Ω–∏—ë–º Dataloader

    Args:
        data_dir: –ø—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –¥–∞–Ω–Ω—ã–º–∏
        img_w_size: —à–∏—Ä–∏–Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ—Å–ª–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–π
        img_h_size: –≤—ã—Å–æ—Ç–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ—Å–ª–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–π
        total_img: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä–æ–µ –Ω—É–∂–Ω–æ
        batch_size: —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–µ–π
        train_ration: –æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∫ –≤—Å–µ–º –¥–∞–Ω–Ω—ã–º
        is_calculate_normalize_dataset: –Ω—É–∂–Ω–æ –ª–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –Ω–∞—à–∏ –¥–∞–Ω–Ω—ã–µ

    Returns:
        Dataloader: Dataloader –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        Dataloader: Dataloader –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—Ö –¥–∞–Ω–Ω—ã—Ö
        list[str]: —Å–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π –∫–ª–∞—Å—Å–æ–≤
    """
    print("‚ö™[load_dataloader] start create dataloaders")

    base_transform = transforms.Compose([
        transforms.Resize((img_h_size, img_w_size)),
        transforms.ToTensor()
    ])

    temp_dataset = datasets.ImageFolder(
        root=data_dir,
        transform=base_transform
    )

    temp_loader = DataLoader(
        temp_dataset,
        batch_size=batch_size,
        shuffle=False,
    )

    classes = temp_loader.dataset.classes

    if is_calculate_normalize_dataset:
        print("üü£[normalize_dataset] processing")
        mean, std = calculate_normalize_datasets(temp_loader)
    
        train_transform = transforms.Compose([
            transforms.RandomResizedCrop(
                size=(img_h_size, img_w_size),
                scale=(0.7, 1.0)
            ),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomRotation(10),
            transforms.ToTensor(),
            transforms.Normalize(mean=mean, std=std)
        ])

        val_transform = transforms.Compose([
            transforms.Resize(size=(img_h_size, img_w_size)),
            transforms.ToTensor(),
            transforms.Normalize(mean=mean, std=std)
        ])
    else:
        train_transform = transforms.Compose([
            transforms.RandomResizedCrop(
                size=(img_h_size, img_w_size),
                scale=(0.7, 1.0)
            ),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomRotation(10),
            transforms.ToTensor(),
        ])

        val_transform = transforms.Compose([
            transforms.Resize(size=(img_h_size, img_w_size)),
            transforms.ToTensor(),
        ])

    if total_img == 0:
        total_img = len(temp_dataset)

    indxs = torch.randperm(len(temp_dataset))[:total_img]
    temp_dataset = Subset(temp_dataset, indxs)

    train_size = int(train_ration * total_img)
    val_size = total_img - train_size

    train_dataset, val_dataset = torch.utils.data.random_split(
        temp_dataset, [train_size, val_size]
    )

    train_dataset.dataset.transform = train_transform
    val_dataset.dataset.transform = val_transform

    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
    )

    print("üü¢[load_dataloader] finish create dataloaders")
    print(f" ‚ûñ Train samples: {len(train_dataset)}")
    print(f" ‚ûñ Val samples:   {len(val_dataset)}")
    print(f" ‚ûñ Classes:       {classes}")

    return train_loader, val_loader, classes

def load_dataloader_detection(
        path: str,
        img_w_size: int = 224,
        img_h_size: int = 224,
        total_img: int = 0,
        batch_size: int = 32,
        train_ration: float = 0.8,
    ) -> Tuple[DataLoader, DataLoader, List[str]]:
    with open(path+'/data.yaml', 'r') as f:
        config = yaml.safe_load(f)

    train_path =  config['train']
    val_path =  config['val']

    classes = config['names']

    train_dataset = DetectionDataset(
        images_dir=train_path,
        global_path=path,
        img_size=(img_h_size, img_w_size)
    )

    val_dataset = DetectionDataset(
        images_dir=val_path,
        global_path=path,
        img_size=(img_h_size, img_w_size)
    )

    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    if total_img > 0:
        train_count = int(total_img * train_ration)
        val_count = total_img - train_count
        
        # –ù–æ –Ω–µ –±–æ–ª—å—à–µ —á–µ–º –µ—Å—Ç—å –≤ –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö
        train_count = min(train_count, len(train_dataset))
        val_count = min(val_count, len(val_dataset))
        
        # –°–æ–∑–¥–∞–µ–º –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–∞
        train_indices = torch.randperm(len(train_dataset))[:train_count]
        val_indices = torch.randperm(len(val_dataset))[:val_count]
        
        train_dataset = Subset(train_dataset, train_indices)
        val_dataset = Subset(val_dataset, val_indices)
    else:
        train_dataset = train_dataset
        val_dataset = val_dataset

    def collate_fn(batch):
        images, labels = zip(*batch)
        images = torch.stack(images)
        return images, labels

    train_dataloader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        collate_fn=collate_fn
    )

    val_dataloader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        collate_fn=collate_fn
    )

    return train_dataloader, val_dataloader, classes

def calculate_normalize_datasets(
        dataloader: DataLoader
    ):
    """
    –í—ã—á–∏—Å–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞

    Args:
        dataloader: –≤–µ—Å—å –∏–∑–≤–µ—Å—Ç–Ω—ã–π –Ω–∞–º –¥–∞—Ç–∞—Å–µ—Ç
    """
    print("‚ö™[calculate_normalize_datasets] start")
    channels_sum = torch.zeros(3)
    channels_sq_sum = torch.zeros(3)
    num_batches = 0

    for data, _ in tqdm(dataloader):
        channels_sum += torch.mean(data, dim=[0,2,3])
        channels_sq_sum +=  torch.mean(data**2, dim=[0,2,3])
        num_batches += 1

    if num_batches == 0:
        raise ValueError("Dataloader –ø—É—Å—Ç")
    
    mean = channels_sum / num_batches
    std = (channels_sq_sum / num_batches - mean**2)**0.5
    print("üü¢[calculate_normalize_datasets] finish")
    return mean, std

def denormalize_image(
        tensor: torch.Tensor, 
        mean: torch.Tensor, 
        std: torch.Tensor,
    ) -> torch.Tensor:
    """
    –î–µ–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è

    Args:
        tensor: –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ –≤–∏–¥–µ —Ç–µ–Ω–∑–æ—Ä–∞
        mean –∏ std: –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –ø—Ä–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏

    Return:
        –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ –≤–∏–¥–µ —Ç–µ–Ω–∑–æ—Ä–∞
    """
    denorm = transforms.Normalize(
        mean=[-m/s for m, s in zip(mean, std)],
        std=[1/s for s in std]
    )
    return denorm(tensor)


def get_images_labels_path(
        images_dir: str, 
        global_path: str,
        verbose: bool = False
    ) -> Tuple[List[str], List[str]]:
    """
    –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ-–º–µ—Ç–∫–∞ –¥–ª—è –∑–∞–¥–∞—á–∏ –¥–µ—Ç–µ–∫—Ü–∏–∏

    Args: 
        images_dir: –ø—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏
        global_path: –±–∞–∑–æ–≤—ã–π –ø—É—Ç—å –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—É—Ç–µ–π
        verbose: –≤—ã–≤–æ–¥–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é 

    Returns:
        –ö–æ—Ä—Ç–µ–∂ (—Å–ø–∏—Å–æ–∫ –ø–∞—Ç—á–µ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, —Å–ø–∏—Å–∫–æ –ø–∞—Ç—á–µ–π –º–µ—Ç–æ–∫)
    """
    if verbose:
        print("üîò[get_images_labels_path] start")

    base_path = images_dir.replace('/images','').replace("..", global_path)
    base_path = Path(base_path)

    path_images = base_path / 'images'
    path_labels = base_path / 'labels'
    
    if not path_images.exists():
        raise FileNotFoundError(f"Dir for images not found: {path_images}")
    if not path_labels.exists:
        raise FileNotFoundError(f"Dir for labels not found: {path_labels}")
    
    image_ext = ['.png', '.jpg', 'jpeg']
    images_paths = []
    
    for ext in image_ext:
        pattern = str(path_images / f'*{ext}')
        images_paths.extend(glob.glob(pattern))
    
    if not images_paths:
        raise ValueError(f"Not found images in {path_images}")
    
    if verbose:
        print("üü§[get_images_labels_path] path has been verified")

    valid_image_paths = []
    valid_label_paths = []
    missing_labels = []

    for img_path in images_paths:
        img_path = Path(img_path)
        img_name = img_path.stem
        labels_paths = path_labels / f"{img_name}.txt"

        if os.path.exists(labels_paths):
            valid_image_paths.append(img_path)
            valid_label_paths.append(str(labels_paths))
        else:
            missing_labels.append(img_name)

    if verbose:
        print(f"üü¢[get_images_labels_path] finish")
        print(f"   - count images:{len(valid_image_paths)}")
        print(f"   - count labels:{len(valid_image_paths)}")
        if missing_labels:
            print(f"   üî¥ missing labels:{len(missing_labels)}")

    return valid_image_paths, valid_label_paths


class DetectionDataset(Dataset):
    """–î–∞—Ç–∞—Å–µ—Ç –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤"""
    
    def __init__(
        self, 
        images_dir: str,
        global_path: str,
        img_size: Tuple[int, int] = (640, 640),
        transform: Any = None,
        verbose: bool = False
    ):
        self.img_size = img_size
        self.transform = transform
        
        self.image_paths, self.label_paths = get_images_labels_path(
            images_dir, global_path, verbose
        )
        
    def __len__(self):
        return len(self.image_paths)
    
    def load_image(self, idx):
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        """
        img_path = self.image_paths[idx]

        image = Image.open(img_path).convert('RGB')
        orig_w, orig_h = image.size

        if not self.transform:
            self.transform = transforms.Compose([
                transforms.Resize(self.img_size),
                transforms.ToTensor()
            ])

        image = self.transform(image)
        return image, (orig_h, orig_w)
    
    def load_labels(self, idx, orig_size):
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫
        """
        label_path = self.label_paths[idx]
        orig_w, orig_h = orig_size
        
        labels = []
        with open(label_path, 'r') as f:
            for line in f.readlines():
                if line.strip():
                    class_id, x_center, y_center, width, height = map(float, line.split())
                    
                    # –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç –∫ –Ω–æ–≤–æ–º—É —Ä–∞–∑–º–µ—Ä—É
                    x_center = x_center * self.img_size[0] / orig_w
                    y_center = y_center * self.img_size[1] / orig_h
                    width = width * self.img_size[0] / orig_w
                    height = height * self.img_size[1] / orig_h
                    
                    labels.append([class_id, x_center, y_center, width, height])
        
        if labels:
            return torch.tensor(labels, dtype=torch.float32)
        else:
            return torch.zeros((0, 5), dtype=torch.float32)
    
    def __getitem__(self, idx):
        image, orig_size = self.load_image(idx)
        labels = self.load_labels(idx, orig_size)
        return image, labels
    